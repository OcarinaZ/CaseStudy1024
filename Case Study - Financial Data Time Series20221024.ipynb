{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we would like to see a demonstration of your ability to use Python to explore and analyse financial data-series, then present your insights.  The project will prompt you to use graphs and data-visualisations at specific points, but please feel free to supplement your submissions with any other visualisations that you feel is appropriate and helpful.  It is organised into 3 sequential sections - we encourage you to read through all the prompts once, first, and consider the context of the entire project, before diving into any code.\n",
    "\n",
    "\n",
    "We intend to focus on (and discuss with you) the following aspects of your project:\n",
    "- Proficiency with the Python standard library (such as I/O operations), data-science libraries (such as pandas and numpy), and data-visualisation libraries (such as matplotlib and ipydatagrid)\n",
    "- Project structure, including defining package pre-requisites and basic environment management (our preferred environment manager is [conda](https://docs.conda.io/en/latest/))\n",
    "- Conciseness and efficiency of solution presented, particularly with respect to the scalability of solution (i.e. considerations for extensions and reuse)\n",
    "  - We are mostly interested in a high level discussion, stopping well short of questions such as \"how quick does the code run\" (see below)\n",
    "- Overall code quality, especially with respect to readability (including quality of documentation and comments, as appropriate)\n",
    "- General comfort with financial instruments, financial data, and markets\n",
    "\n",
    "\n",
    "We **do not** intend to focus on the following aspects of your project:\n",
    "- Novel or interesting algorithms to perform any of the analyses\n",
    "  - This includes computational optimisations or parallelisations considerations\n",
    "- Specific details of code-formatting and code-style (other than consistency throughout the project)\n",
    "  - Please use as many code cells as you would like and organise your project to best fit your style, so long as it generally follows the structure and flow of the prompts\n",
    "- Macro, fundamental, or technical analysis of any financial instrument beyond any discussions indicated in the prompts\n",
    "\n",
    "\n",
    "If you are familiar and comfortable with git, we *strongly* recommend that you start a git repository for the project and submit the entire repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "# Setup - Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please ensure that your environment is capable of running this Jupyter workbook as well as any python modules and scripts.  The requirements are:\n",
    "\n",
    "- python >= 3.8\n",
    "- jupyterlab\n",
    "- pandas >= 1.2; optional dependancies we will need are:\n",
    "  - openpyxl\n",
    "  - matplotlib\n",
    "- requests\n",
    "- [yfinance](https://github.com/ranaroussi/yfinance)\n",
    "\n",
    "\n",
    "If you have any other requirements for your submission, please list them below.  Additionally, if you created a new environment specifically for this project, please include the `requirements.txt`, `environment.yml`, or other similar requirements/configuration file, and specify the environment manager you used below; please also add relevant any notes or comments that you think may be helpful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-23T09:09:12.500750Z",
     "start_time": "2022-10-23T09:09:04.621580Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import yfinance as yf\n",
    "from datetime import datetime\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1. Initial Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the yfinance package to retrieve price history data for the following stock market indices and stocks:\n",
    "- SPY\n",
    "- AGG\n",
    "- F\n",
    "- GM\n",
    "- UBER\n",
    "- TSLA\n",
    "- GOOG\n",
    "- SNAP\n",
    "- VOW3.DE\n",
    "- EXSA.DE\n",
    "- EURUSD=X\n",
    "- GD=F\n",
    "\n",
    "\n",
    "Additionally, we would like you to add to the list 4 more tickers, consisting of:\n",
    "- Any **2** tickers for companies domiciled in Europe, and priced in EUR\n",
    "- Any **2** tickers for companies domiciled in the US, and priced in USD\n",
    "\n",
    "\n",
    "The time period for our analysis will be from **Dec 31, 2015 to Dec 31, 2021**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt 1: Retrieving and caching data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the [yfinance documentation](https://github.com/ranaroussi/yfinance) as a guide, please retrieve historical data from Yahoo Finance for the 16 instruments listed above.  Please save the raw data download (without any data cleaning or processing) as a `pickle` in your project directory.  Please use the pickled pandas object as a cache for the rest of the project and re-load the data you require from the pickle, if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-23T12:51:23.151970Z",
     "start_time": "2022-10-23T12:51:23.133980Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_raw_data(ticker_list,start_date,end_date):\n",
    "\n",
    "    '''\n",
    "    Function for getting raw historical data from Yahoo Finance\n",
    "    '''\n",
    "    \n",
    "    data = yf.download(ticker_list,start=start_date,end=end_date)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-23T12:51:27.071723Z",
     "start_time": "2022-10-23T12:51:27.043741Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# parameter settings\n",
    "\n",
    "# -- default ticker list\n",
    "ticker_list = (['SPY','AGG','F','GM','UBER','TSLA','GOOG','SNAP','VOW3.DE','EXSA.DE','EURUSD=X','GD=F'])\n",
    "# -- add-on ticker list: 2 EUR stocks & 2 USD stocks\n",
    "ticker_list_added = (['SIE.DE','MBG.DE','TM','AAPL'])\n",
    "\n",
    "ticker_list = ticker_list + ticker_list_added\n",
    "\n",
    "# -- dates\n",
    "start_date = datetime(2015,12,31)\n",
    "end_date = datetime(2021,12,31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-23T12:51:56.713734Z",
     "start_time": "2022-10-23T12:51:53.852371Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  16 of 16 completed\n"
     ]
    }
   ],
   "source": [
    "# save raw data to pickle file\n",
    "raw_data = get_raw_data(ticker_list,start_date,end_date)\n",
    "pd.to_pickle(raw_data,'raw_data_16tickers.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prompt 2: Data wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data clean-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please realign the data index as a daily data-series and please extract just the \"Adj Close\" data; i.e. the reshaped dataframe should have dates (without time) as its index and tickers (e.g. `SPY`, `GD=F`) as its columns.  In either a markdown box or in comments, please note any assumptions or adjustments you may have made.  Please export this data to a `.csv` file in your project directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-23T12:52:41.611654Z",
     "start_time": "2022-10-23T12:52:41.403774Z"
    }
   },
   "outputs": [],
   "source": [
    "# reload raw data\n",
    "raw_data = pd.read_pickle('raw_data_16tickers.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some exploration of null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-23T12:53:13.839862Z",
     "start_time": "2022-10-23T12:53:13.772902Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AAPL</th>\n",
       "      <th>AGG</th>\n",
       "      <th>EURUSD=X</th>\n",
       "      <th>EXSA.DE</th>\n",
       "      <th>F</th>\n",
       "      <th>GD=F</th>\n",
       "      <th>GM</th>\n",
       "      <th>GOOG</th>\n",
       "      <th>MBG.DE</th>\n",
       "      <th>SIE.DE</th>\n",
       "      <th>SNAP</th>\n",
       "      <th>SPY</th>\n",
       "      <th>TM</th>\n",
       "      <th>TSLA</th>\n",
       "      <th>UBER</th>\n",
       "      <th>VOW3.DE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-12-31</th>\n",
       "      <td>24.130863</td>\n",
       "      <td>92.144112</td>\n",
       "      <td>1.093398</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.446766</td>\n",
       "      <td>312.299988</td>\n",
       "      <td>28.295013</td>\n",
       "      <td>37.944000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>180.575638</td>\n",
       "      <td>118.950439</td>\n",
       "      <td>16.000668</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.085906</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-04</th>\n",
       "      <td>24.151495</td>\n",
       "      <td>92.109970</td>\n",
       "      <td>1.085399</td>\n",
       "      <td>35.750000</td>\n",
       "      <td>10.357795</td>\n",
       "      <td>309.799988</td>\n",
       "      <td>27.712639</td>\n",
       "      <td>37.091999</td>\n",
       "      <td>50.609890</td>\n",
       "      <td>68.110130</td>\n",
       "      <td>NaN</td>\n",
       "      <td>178.051300</td>\n",
       "      <td>117.422958</td>\n",
       "      <td>14.894000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>105.132751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-05</th>\n",
       "      <td>23.546274</td>\n",
       "      <td>92.152626</td>\n",
       "      <td>1.082755</td>\n",
       "      <td>36.029999</td>\n",
       "      <td>10.172438</td>\n",
       "      <td>307.250000</td>\n",
       "      <td>26.980511</td>\n",
       "      <td>37.129002</td>\n",
       "      <td>50.603016</td>\n",
       "      <td>68.540146</td>\n",
       "      <td>NaN</td>\n",
       "      <td>178.352417</td>\n",
       "      <td>117.113594</td>\n",
       "      <td>14.895333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.974022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-06</th>\n",
       "      <td>23.085482</td>\n",
       "      <td>92.502426</td>\n",
       "      <td>1.075199</td>\n",
       "      <td>35.529999</td>\n",
       "      <td>9.720162</td>\n",
       "      <td>297.750000</td>\n",
       "      <td>26.015438</td>\n",
       "      <td>37.181000</td>\n",
       "      <td>49.317844</td>\n",
       "      <td>68.269394</td>\n",
       "      <td>NaN</td>\n",
       "      <td>176.102646</td>\n",
       "      <td>114.445328</td>\n",
       "      <td>14.602667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>98.894653</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 AAPL        AGG  EURUSD=X    EXSA.DE          F        GD=F  \\\n",
       "Date                                                                           \n",
       "2015-12-31  24.130863  92.144112  1.093398        NaN  10.446766  312.299988   \n",
       "2016-01-01        NaN        NaN  1.085906        NaN        NaN         NaN   \n",
       "2016-01-04  24.151495  92.109970  1.085399  35.750000  10.357795  309.799988   \n",
       "2016-01-05  23.546274  92.152626  1.082755  36.029999  10.172438  307.250000   \n",
       "2016-01-06  23.085482  92.502426  1.075199  35.529999   9.720162  297.750000   \n",
       "\n",
       "                   GM       GOOG     MBG.DE     SIE.DE  SNAP         SPY  \\\n",
       "Date                                                                       \n",
       "2015-12-31  28.295013  37.944000        NaN        NaN   NaN  180.575638   \n",
       "2016-01-01        NaN        NaN        NaN        NaN   NaN         NaN   \n",
       "2016-01-04  27.712639  37.091999  50.609890  68.110130   NaN  178.051300   \n",
       "2016-01-05  26.980511  37.129002  50.603016  68.540146   NaN  178.352417   \n",
       "2016-01-06  26.015438  37.181000  49.317844  68.269394   NaN  176.102646   \n",
       "\n",
       "                    TM       TSLA  UBER     VOW3.DE  \n",
       "Date                                                 \n",
       "2015-12-31  118.950439  16.000668   NaN         NaN  \n",
       "2016-01-01         NaN        NaN   NaN         NaN  \n",
       "2016-01-04  117.422958  14.894000   NaN  105.132751  \n",
       "2016-01-05  117.113594  14.895333   NaN  100.974022  \n",
       "2016-01-06  114.445328  14.602667   NaN   98.894653  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \n",
    "prices = raw_data['Adj Close'].reset_index().set_index('Date')\n",
    "prices.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AAPL         56\n",
       "AGG          56\n",
       "EURUSD=X      3\n",
       "EXSA.DE      46\n",
       "F            56\n",
       "GD=F        236\n",
       "GM           56\n",
       "GOOG         56\n",
       "MBG.DE       45\n",
       "SIE.DE       45\n",
       "SNAP        349\n",
       "SPY          56\n",
       "TM           56\n",
       "TSLA         56\n",
       "UBER        900\n",
       "VOW3.DE      45\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prices.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "\n",
    "Some comments on missing value processing:\n",
    "\n",
    "</font>\n",
    "\n",
    "The reason why `UBER` and `SNAP` have so many missing values is that `UBER`'s IPO was in 2019, while `SNAP`'s was in 2018, thus both of them don't have data available until then. Since in all there are only around 1500 rows, excluding 900 rows is not a good idea. Instead, here we use the following method to fill the missing values:\n",
    "\n",
    "1. Use `ffill` with limit on maximum number ($\\leq3$) of consecutive NaNs and fill the scattered missing values;\n",
    "2. For tickers with too many missing values after step 1, exclude them from the subsets in `dropna` function;\n",
    "3. Drop rows containing NaNs in other tickers.\n",
    "\n",
    "As a result, the length of the data set is kept, while scattered NaNs are filled, which is proper for further data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-23T12:09:51.759598Z",
     "start_time": "2022-10-23T12:09:51.725620Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_up(raw_data, fill_thresh = 3, keep_NaN_thresh = 3):\n",
    "    \n",
    "    '''Function for data cleaning.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    raw_data: DataFrame\n",
    "              Dataframe containing raw historical data without any data cleaning or processing.\n",
    "    \n",
    "    fill_thresh: int\n",
    "                 Maximum number of consecutive NaNs to fill.\n",
    "    \n",
    "    keep_NaN_thresh: int\n",
    "                     Exclude a ticker from subset for NaN-detecting if the total NaN count exceeds keep_NaN_thresh.\n",
    "                     \n",
    "                     \n",
    "    Returns\n",
    "    -------\n",
    "    cleaned_prices: DataFrame\n",
    "                    Cleaned Adj Close with date format index.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    prices = raw_data['Adj Close'].reset_index().set_index('Date')\n",
    "    prices.index = prices.index.date\n",
    "    \n",
    "    # Fill scattered NaNs with maximum consecutive NaNs limit\n",
    "    cleaned_prices = prices.fillna(method='ffill',limit=fill_thresh)\n",
    "    \n",
    "    # Find the ticker subset which will be further used to dropna\n",
    "    NaN_check = cleaned_prices.isna().sum().to_dict().items()\n",
    "    exemption_tick = [key for key, value in NaN_check if value > keep_NaN_thresh]\n",
    "    \n",
    "    cleaned_prices.dropna(inplace=True,subset=[ticker for ticker in list(cleaned_prices.columns) \n",
    "                                               if ticker not in exemption_tick])\n",
    "    \n",
    "    return cleaned_prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-23T12:10:03.208414Z",
     "start_time": "2022-10-23T12:10:03.185426Z"
    }
   },
   "outputs": [],
   "source": [
    "cleaned_prices = clean_up(raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "Check NaN count for each ticker after data clean-up:    \n",
    "</font>\n",
    "\n",
    "All scattered NaNs are filled, while the consecutive NaNs are kept, which will be specifically processed according to the specific problem later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-23T12:10:05.332666Z",
     "start_time": "2022-10-23T12:10:05.305499Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AAPL          0\n",
       "AGG           0\n",
       "EURUSD=X      0\n",
       "EXSA.DE       0\n",
       "F             0\n",
       "GD=F        131\n",
       "GM            0\n",
       "GOOG          0\n",
       "MBG.DE        0\n",
       "SIE.DE        0\n",
       "SNAP        303\n",
       "SPY           0\n",
       "TM            0\n",
       "TSLA          0\n",
       "UBER        874\n",
       "VOW3.DE       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NaN_count = cleaned_prices.isna().sum()\n",
    "NaN_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-23T12:10:10.056838Z",
     "start_time": "2022-10-23T12:10:09.893831Z"
    }
   },
   "outputs": [],
   "source": [
    "# export cleaned_prices\n",
    "cleaned_prices.to_csv('cleaned_prices_16tickers.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, in a separate `.py` file, please write a function to read the `.csv` file saved in the last section, and generate some descriptive statistics.  Please compute at least: minimum, maximum, and mean Adj Close as well as total return over the entire data period; please feel free to compute and show a few (2 or 3) additional statistics as well if you would like to fill out the table.\n",
    "\n",
    "\n",
    "Specifically, the function should accept as input:\n",
    "  ```\n",
    "  filepath: str and/or Path\n",
    "    File path to a .csv data with time-series data\n",
    "  ```\n",
    "\n",
    "\n",
    "and returns:\n",
    "  ```\n",
    "  Pandas dataframe of descriptive statistics; the index should be tickers, matching the columns in the input csv file, and the columns should be appropriate labels for the descriptive statistics you are tabulating \n",
    "  ```\n",
    "  \n",
    "Please specify the filename of the `.py` file you have written:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "\n",
    "des_stats_16tickers.py\n",
    "    \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please import the function you have just written, pass the `.csv` file you generated earlier on to it, and display the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-23T09:09:12.803397Z",
     "start_time": "2022-10-23T09:09:12.697458Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AAPL</th>\n",
       "      <th>AGG</th>\n",
       "      <th>EURUSD=X</th>\n",
       "      <th>EXSA.DE</th>\n",
       "      <th>F</th>\n",
       "      <th>GD=F</th>\n",
       "      <th>GM</th>\n",
       "      <th>GOOG</th>\n",
       "      <th>MBG.DE</th>\n",
       "      <th>SIE.DE</th>\n",
       "      <th>SNAP</th>\n",
       "      <th>SPY</th>\n",
       "      <th>TM</th>\n",
       "      <th>TSLA</th>\n",
       "      <th>UBER</th>\n",
       "      <th>VOW3.DE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-04</th>\n",
       "      <td>24.151495</td>\n",
       "      <td>92.109970</td>\n",
       "      <td>1.085399</td>\n",
       "      <td>35.750000</td>\n",
       "      <td>10.357795</td>\n",
       "      <td>309.799988</td>\n",
       "      <td>27.712639</td>\n",
       "      <td>37.091999</td>\n",
       "      <td>50.609890</td>\n",
       "      <td>68.110130</td>\n",
       "      <td>NaN</td>\n",
       "      <td>178.051300</td>\n",
       "      <td>117.422958</td>\n",
       "      <td>14.894000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>105.132751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-05</th>\n",
       "      <td>23.546274</td>\n",
       "      <td>92.152626</td>\n",
       "      <td>1.082755</td>\n",
       "      <td>36.029999</td>\n",
       "      <td>10.172438</td>\n",
       "      <td>307.250000</td>\n",
       "      <td>26.980511</td>\n",
       "      <td>37.129002</td>\n",
       "      <td>50.603016</td>\n",
       "      <td>68.540146</td>\n",
       "      <td>NaN</td>\n",
       "      <td>178.352417</td>\n",
       "      <td>117.113594</td>\n",
       "      <td>14.895333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.974022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-06</th>\n",
       "      <td>23.085482</td>\n",
       "      <td>92.502426</td>\n",
       "      <td>1.075199</td>\n",
       "      <td>35.529999</td>\n",
       "      <td>9.720162</td>\n",
       "      <td>297.750000</td>\n",
       "      <td>26.015438</td>\n",
       "      <td>37.181000</td>\n",
       "      <td>49.317844</td>\n",
       "      <td>68.269394</td>\n",
       "      <td>NaN</td>\n",
       "      <td>176.102646</td>\n",
       "      <td>114.445328</td>\n",
       "      <td>14.602667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>98.894653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-07</th>\n",
       "      <td>22.111172</td>\n",
       "      <td>92.493896</td>\n",
       "      <td>1.077900</td>\n",
       "      <td>34.689999</td>\n",
       "      <td>9.416174</td>\n",
       "      <td>295.049988</td>\n",
       "      <td>24.950527</td>\n",
       "      <td>36.319500</td>\n",
       "      <td>47.421001</td>\n",
       "      <td>66.963402</td>\n",
       "      <td>NaN</td>\n",
       "      <td>171.877640</td>\n",
       "      <td>111.728729</td>\n",
       "      <td>14.376667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>95.650841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-08</th>\n",
       "      <td>22.228090</td>\n",
       "      <td>92.698608</td>\n",
       "      <td>1.092598</td>\n",
       "      <td>34.160000</td>\n",
       "      <td>9.297547</td>\n",
       "      <td>295.000000</td>\n",
       "      <td>24.567829</td>\n",
       "      <td>35.723499</td>\n",
       "      <td>46.864319</td>\n",
       "      <td>66.501534</td>\n",
       "      <td>NaN</td>\n",
       "      <td>169.991074</td>\n",
       "      <td>109.302155</td>\n",
       "      <td>14.066667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>95.734024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 AAPL        AGG  EURUSD=X    EXSA.DE          F        GD=F  \\\n",
       "2016-01-04  24.151495  92.109970  1.085399  35.750000  10.357795  309.799988   \n",
       "2016-01-05  23.546274  92.152626  1.082755  36.029999  10.172438  307.250000   \n",
       "2016-01-06  23.085482  92.502426  1.075199  35.529999   9.720162  297.750000   \n",
       "2016-01-07  22.111172  92.493896  1.077900  34.689999   9.416174  295.049988   \n",
       "2016-01-08  22.228090  92.698608  1.092598  34.160000   9.297547  295.000000   \n",
       "\n",
       "                   GM       GOOG     MBG.DE     SIE.DE  SNAP         SPY  \\\n",
       "2016-01-04  27.712639  37.091999  50.609890  68.110130   NaN  178.051300   \n",
       "2016-01-05  26.980511  37.129002  50.603016  68.540146   NaN  178.352417   \n",
       "2016-01-06  26.015438  37.181000  49.317844  68.269394   NaN  176.102646   \n",
       "2016-01-07  24.950527  36.319500  47.421001  66.963402   NaN  171.877640   \n",
       "2016-01-08  24.567829  35.723499  46.864319  66.501534   NaN  169.991074   \n",
       "\n",
       "                    TM       TSLA  UBER     VOW3.DE  \n",
       "2016-01-04  117.422958  14.894000   NaN  105.132751  \n",
       "2016-01-05  117.113594  14.895333   NaN  100.974022  \n",
       "2016-01-06  114.445328  14.602667   NaN   98.894653  \n",
       "2016-01-07  111.728729  14.376667   NaN   95.650841  \n",
       "2016-01-08  109.302155  14.066667   NaN   95.734024  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prices = pd.read_csv('cleaned_prices_16tickers.csv',index_col=0)\n",
    "prices.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-23T10:37:19.294789Z",
     "start_time": "2022-10-23T10:37:19.102898Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'des_stats_16tickers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/0v/91c9prmn50l3wp_blf95j_jw0000gn/T/ipykernel_64219/3177802455.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdes_stats_16tickers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdes_stats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mPATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'cleaned_prices_16tickers.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdes_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdes_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdes_table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'des_stats_16tickers'"
     ]
    }
   ],
   "source": [
    "from des_stats_16tickers import des_stats\n",
    "PATH = 'cleaned_prices_16tickers.csv'\n",
    "des_table = des_stats(PATH)\n",
    "des_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2. Bollinger Bands Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Bollinger Band is an analytical tool used to study the prices of financial instruments.  Please refer to this [Investopedia article](https://www.investopedia.com/terms/b/bollingerbands.asp) as well as this [Wikipedia article](https://en.wikipedia.org/wiki/Bollinger_Bands), for example, for some common explanations for the definition and interpretation of this analysis.\n",
    "\n",
    "\n",
    "In this section, we will adapt the definition of the Bollinger Band slightly to fit the data we have.  Specifically, let\n",
    "\\begin{align}\n",
    "P_t &= \\mbox{Adj Close on trading day }\\ t, \\\\\n",
    "MA\\ (k)_t &= \\mbox{Moving Average of }\\ P_t \\mbox{ over the previous }\\ k \\mbox{ trading days}, \\\\\n",
    "\\sigma\\ (k)_t &= \\mbox{Standard Deviation of }\\ P_t \\mbox{ over the previous }\\ k \\mbox{ trading days} \\\\\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "The $Bollinger\\ (k,N)$ series for a financial instrument consists of 4 data series,\n",
    "\\begin{align}\n",
    "P_t&, \\\\\n",
    "MA\\ (k)_t&, \\\\\n",
    "UB\\ (k,N) &= MA\\ (k)_t + N*\\sigma\\ (k)_t, \\\\\n",
    "\\mbox{and }\\ LB\\ (k,N) &= MA\\ (k)_t - N*\\sigma\\ (k)_t \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt 3: Calculating Bollinger Bands\n",
    "Please calculate the $Bollinger\\ (20,2)$ series for `SPY`, and display the results graphically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-23T13:14:55.285330Z",
     "start_time": "2022-10-23T13:14:55.233361Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook  \n",
    "\n",
    "def bollinger_bands(ticker,cleaned_prices,k,N,plot_flag=True):\n",
    "    \n",
    "    price = cleaned_prices[ticker]\n",
    "    \n",
    "    MA = price.rolling(k).mean()\n",
    "    UB = MA + N * price.rolling(k).std()\n",
    "    LB = MA - N * price.rolling(k).std()\n",
    "    \n",
    "    bbands = pd.DataFrame({'Price_'+ ticker: price,\n",
    "                          'MA_'+ ticker: MA,\n",
    "                          'UB_'+ ticker: UB,\n",
    "                          'LB_'+ ticker: LB}).dropna()\n",
    "    \n",
    "    # For plotting\n",
    "    if plot_flag:\n",
    "        ax = bbands.plot()\n",
    "        ax.set_title(f'Bollinger Bands For {ticker}, k={k},N={N}',fontsize=18)\n",
    "        ax.set_xlabel('Date',fontsize=12)\n",
    "        ax.set_ylabel('Price',fontsize=12)\n",
    "    \n",
    "    return bbands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-23T13:15:30.618336Z",
     "start_time": "2022-10-23T13:15:30.090639Z"
    }
   },
   "outputs": [],
   "source": [
    "_ = bollinger_bands('SPY',cleaned_prices,20,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please calculate 2 more data series, the $Bollinger\\ (60,1.5)$ series for the `SPY`, and the $Bollinger\\ (k,N)$ series for the `SPY` for a different pair of any $(k,N)$ of your choice. Please visualise your findings, and referring to those graphs, provide some brief written comments on the following questions:\n",
    "- Please describe the relationship between the parameters $(k,N)$ and the analytical results\n",
    "- What statistical attributes for any given price time-series would potentially make the Bollinger Bands less and more useful as the basis for a trading strategy?\n",
    "\n",
    "\n",
    "**Please be prepared to discuss these results and comments further**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "Relationship between (k,N) and the analytical results:\n",
    "</font>\n",
    "\n",
    "Higher k leads to smoother MA/UB/LB curve; higher N leads to broader yet more volatile UB/LB, which makes sense since N controls how many standard deviations are included between UB and LB.\n",
    "\n",
    "<br>\n",
    "\n",
    "<font color='blue'>\n",
    "Statistical attributes:\n",
    "</font>\n",
    "\n",
    "1. The proportion of prices above UB or below LB should be similar to the \"tail\" probability of a distribution. E.g. for randomly distributed X, the probability of X's value lying between $\\pm2\\sigma$ is about 95.6%. Thus, a Bollinger Band is more reasonable if 95.6% of the prices lie within UB and LB.\n",
    "\n",
    "<div>\n",
    "<img src=\"https://cdn.scribbr.com/wp-content/uploads/2020/10/normal-distribution.png\", width=320, heigth=240>\n",
    "</div>\n",
    "\n",
    "2. The std-to-mean ratio should be relatively large, so that the buy/sell signals can be more significant and the constructed strategy can be both more reliable, more executable and more likely to make a profit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-23T13:18:31.981804Z",
     "start_time": "2022-10-23T13:18:31.817894Z"
    }
   },
   "outputs": [],
   "source": [
    "_ = bollinger_bands('SPY',cleaned_prices,60,1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-23T11:25:43.088458Z",
     "start_time": "2022-10-23T11:25:42.917557Z"
    }
   },
   "outputs": [],
   "source": [
    "_ = bollinger_bands('SPY',cleaned_prices,10,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt 4: Extending the analysis\n",
    "One way to think about how useful a particular $Bollinger\\ (k,N)$ series may be as a trading signal is to consider how often the price series stays within $UB\\ (k,N)$ and $LB\\ (k,N)$.  Please propose a calculation and simple algorithm to capture this statistic, which we can denote as $W\\ (k,N) = \\mbox{Percentage of of the data series }P_t,\\ such\\ that\\ LB\\ (k,N) < P_t < UB\\ (k,N)$.  Then, please choose **any 4 distinct pairs of $(k,N)$ as well as the pair $(20,2)$** (i.e. 5 pairs), and compute the corresponding $W\\ (k,N)$ for `SPY` and present them in a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-23T12:10:28.765283Z",
     "start_time": "2022-10-23T12:10:28.749292Z"
    }
   },
   "outputs": [],
   "source": [
    "def cal_pct_within(ticker,cleaned_prices,k,N,plot_flag):\n",
    "    \n",
    "    bbands = bollinger_bands(ticker,cleaned_prices,k,N,plot_flag)\n",
    "    W = (sum((bbands['LB_'+ticker]<bbands['Price_'+ticker]) & (bbands['Price_'+ticker]<bbands['UB_'+ticker]))\n",
    "         /len(bbands))\n",
    "    \n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-23T12:11:15.516370Z",
     "start_time": "2022-10-23T12:11:14.558920Z"
    }
   },
   "outputs": [],
   "source": [
    "k_list = [10,20,30,60,60]\n",
    "N_list = [3,2,1.5,1.5,3]\n",
    "\n",
    "W_table = pd.DataFrame({'k':k_list,'N':N_list})\n",
    "for i,tick in enumerate(cleaned_prices.columns.values): # cleaned_prices.columns.values = tick_list\n",
    "    # print(i,tick)\n",
    "    W_list = []\n",
    "    for i in range(len(k_list)):\n",
    "        W_list.append(cal_pct_within(tick,cleaned_prices,k_list[i],N_list[i],False))\n",
    "    W_table['W_'+tick] = W_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-23T12:11:22.617303Z",
     "start_time": "2022-10-23T12:11:22.555340Z"
    }
   },
   "outputs": [],
   "source": [
    "W_table[['k','N','W_SPY']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross tabulate $W\\ (k,N)$ for those same 5 pairs of $(k,N)$ for all 16 tickers we have used for this exercise, and provide some brief written comments to address the following questions:\n",
    "- Qualitatively describe how you would design an algorithm to arrive at the optimal $W\\ (k,N)$\n",
    "- Qualitatively propose other metrics that you may use to evaluate how useful any particular $Bollinger\\ (k,N)$ series is\n",
    "\n",
    "\n",
    "**Please be prepared to discuss these results and comments further**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "Arrive at the optimal W(k,N):\n",
    "</font>\n",
    "\n",
    "Similar to 'What would potentially make the Bollinger Bands less and more useful', the optimal (k, N) should satisfy both \n",
    "1. W should be similar to 'non-outlier' proportion of a specific distribution; \n",
    "2. std-to-mean should be relatively large.\n",
    "\n",
    "Thus, the algorithm should be: \n",
    "1. Determine the target non-outlier proportion & std-to-mean;\n",
    "2. For different (k,N) combinations, find the (k,N) which has the closest non-outlier proportion & std-to-mean compared to the target;\n",
    "3. If necessary, backtest and calculate the PNL and try on different thresholds in step 1.\n",
    "\n",
    "<br>\n",
    "<font color='blue'>\n",
    "Other metrics that can be used in market timing:\n",
    "</font>\n",
    "\n",
    "1. The correlation between $W_{ML}$(the proportion of prices lying between MA & LB) and $R_{t+\\Delta t}$(return of following period with a specific window length);\n",
    "2. The correlation between $W_{UM}$(the proportion of prices lying between UB & MA) and $R_{t+\\Delta t}$(return of following period with a specific window length)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_[**Note - this is entirely optional:** If there are any other insights that you have gleaned in this section about Bollinger Bands or the tickers we have used for this exercise, please feel free to add to this section below and present your findings.]_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "The time when the relationship between $W_{ML}$ and $W_{UM}$ change from $W_{ML}<W_{UM}$ to $W_{ML}>W_{UM}$ might be a signal for trading strategy.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3. Statistical attributes of financial data series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often, it can be useful to transform price series into return series (and in fact, the vast majority of academic literature addressing financial instruments is written based on returns series analysis). In this section, we will transform the data we have and perform some simple statistical exercises and take some first steps towards thinking about portfolio construction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt 5: Computing daily returns series\n",
    "\n",
    "\n",
    "\n",
    "We want to make sure that we are measuring returns for all the tickers on the same currency basis.  Specifically, `VOW3.DE`, `EXSA.DE`, and the two European tickers that you have chosen have price series denominated in `EUR$` Fortunately for us, we have the `EURUSD=F` ticker, which gives us the `EUR`-to-`USD` exchange rate each day (i.e. the Adj Close for `EURUSD=F` tells you how many `USD$` `EUR$1.00` will buy that day).  Please compute a new dataframe, `Adj USD Close`, which is the `Adj Close` dataframe denominated in `USD$`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-23T12:28:45.480378Z",
     "start_time": "2022-10-23T12:28:45.452395Z"
    }
   },
   "outputs": [],
   "source": [
    "cleaned_USD_prices = cleaned_prices.copy()\n",
    "for col in cleaned_prices.columns:\n",
    "    # select EUR tickers\n",
    "    if '.DE' in col:\n",
    "        cleaned_USD_prices[col] = cleaned_prices[col]*cleaned_prices['EURUSD=X']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-23T12:28:47.898591Z",
     "start_time": "2022-10-23T12:28:47.856615Z"
    }
   },
   "outputs": [],
   "source": [
    "cleaned_USD_prices.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please generate a dataframe, `Ret`, for the daily returns for all 16 tickers in our new `Adj USD Close` dataset (where, to be specific, $Ret_t=\\frac{P_t}{P_{t-1}}-1$), and please tabulate the following descriptive statistics for each ticker:\n",
    "- Annualised return\n",
    "- Annualised standard deviation of daily returns\n",
    "\n",
    "Please feel free to add a few more (no more than 3) statistics that you think may be interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-23T15:13:50.512593Z",
     "start_time": "2022-10-23T15:13:50.424643Z"
    }
   },
   "outputs": [],
   "source": [
    "Ret = cleaned_USD_prices.pct_change()\n",
    "Ret.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-23T17:18:37.658172Z",
     "start_time": "2022-10-23T17:18:37.181445Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from empyrical.stats import annual_return,annual_volatility,max_drawdown,sharpe_ratio,sortino_ratio\n",
    "\n",
    "def Annualised_return(ret):\n",
    "    \n",
    "    ret_ = ret.loc[ret.index>=ret.isna().idxmin()]     \n",
    "    num_years = (len(ret_)+ 1)/252\n",
    "    cumreturn = (ret_+1).cumprod()[-1]\n",
    "    Annualised_return = (cumreturn)**(1/num_years) - 1\n",
    "    \n",
    "    return Annualised_return \n",
    "\n",
    "def Annualised_standard_deviation(ret):\n",
    "    Annualised_std = np.nanstd(ret)*np.sqrt(252)\n",
    "    return Annualised_std\n",
    "\n",
    "\n",
    "SIMPLE_STAT_FUNCS = [Annualised_return,Annualised_standard_deviation,max_drawdown,sharpe_ratio,sortino_ratio]\n",
    "STAT_FUNC_NAMES = ['Annualised return','Annualised std','Max drawdown','Sharpe ratio','Sortino ratio'] \n",
    "\n",
    "def perf_stats_(ret):\n",
    "    stats = pd.Series(dtype='float64')\n",
    "    for stat_name,stat_func in zip(STAT_FUNC_NAMES,SIMPLE_STAT_FUNCS):\n",
    "        stats[stat_name] = stat_func(ret.dropna())\n",
    "    return stats\n",
    "\n",
    "RET_statistics = pd.DataFrame()\n",
    "for i,tick in enumerate(cleaned_prices.columns.values): \n",
    "    #print(i,tick)\n",
    "    df1 = perf_stats_(Ret[tick]).to_frame(name=tick)\n",
    "    RET_statistics = pd.concat([RET_statistics,df1],axis=1)\n",
    "RET_statistics = RET_statistics.T\n",
    "RET_statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt 6: Describing relationships between returns series\n",
    "\n",
    "\n",
    "One useful way to think about the relationship between financial instruments is to analyse the correlation between their daily returns series.  Please compute the correlation matrix for the full data-series in the `Ret` dataframe and tabulate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-23T15:14:11.027393Z",
     "start_time": "2022-10-23T15:14:10.956436Z"
    }
   },
   "outputs": [],
   "source": [
    "Ret.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you could have only picked 4 ticker out of the 16 to hold in an investment portfolio, in equal weight, over the period of the data, which 4 tickers would likely result in a portfolio with lowest volatility (i.e. lower standard deviation of daily returns)?  _Although we intend to discuss this question mostly from a qualitative perspective, please feel free to substantiate your comments with further quantitative analysis, including graphs and tables, if you feel it would be helpful._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "Qualitative intuition: \n",
    "</font>\n",
    "Tickers with negative correlation are more likely to form a portfolio with lowest volatility.\n",
    "\n",
    "<font color='blue'>\n",
    "<br>\n",
    "    \n",
    "Quantitative analysis:\n",
    "</font>\n",
    "\n",
    "1. Get all the combinations of 4 tickers out of 16.\n",
    "2. For each combination, calculate the equally weighted portfolio return, and then calculate the std of the portfolio.\n",
    "3. Select the combination with lowest portfolio return std.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "combs = combinations(Ret.columns,4)\n",
    "min_std = float('inf')\n",
    "min_comb = []\n",
    "for comb in list(combs):\n",
    "    portfolio_std = Ret[list(comb)].mean(axis=1).std()\n",
    "    if min_std > portfolio_std:\n",
    "        min_std = portfolio_std\n",
    "        min_comb = list(comb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_comb, min_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "From the correlation matrix below, we can see the qualitative intuition corresponds with the quantitative analysis result.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ret[min_comb].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can sometimes study the correlations characteristics of a particular ticker, say for instance for a new company we are interesting in analysing, to make some \n",
    "simplifying high-level assumptions.  Please graph the rolling 6-month correlations of `TSLA` to `F`, `GM`, `GOOG`, and `SNAP`, and provide brief written comments to address the following question:\n",
    "- Given that `F` and `GM` are car manufacturing companies while `GOOG` and `SNAP` are technology services companies, what industry would you think `TSLA` falls into (it does not have to be either of the two industries spanned by the other 4 tickers), and why?\n",
    "\n",
    "\n",
    "**Please be prepared to discuss these results and comments further**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-24T00:59:55.183017Z",
     "start_time": "2022-10-24T00:59:55.059089Z"
    }
   },
   "outputs": [],
   "source": [
    "corr_TSLA = Ret[['TSLA','F','GM','GOOG','SNAP']].rolling(120).corr().dropna().swaplevel().loc['TSLA']\n",
    "corr_TSLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-24T00:50:50.540616Z",
     "start_time": "2022-10-24T00:50:50.313750Z"
    }
   },
   "outputs": [],
   "source": [
    "corr_TSLA.drop(columns=['TSLA']).plot(title='Rolling 6-month Correlation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "Method for classifying:\n",
    "</font>\n",
    "\n",
    "1. For each rolling window, find the stock which has the maximum correlation with `TSLA`.\n",
    "2. Calculate the proportion of each ticker appearing as the maximum_correlation_ticker.\n",
    "3. `TSLA` should be more similar to the industry whose stocks have a larger correlation with `TSLA` for most of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-24T01:00:00.373475Z",
     "start_time": "2022-10-24T01:00:00.329497Z"
    }
   },
   "outputs": [],
   "source": [
    "corr_TSLA = corr_TSLA.drop(columns=['TSLA'])\n",
    "corr_TSLA['MaxCorrTicker'] = corr_TSLA.idxmax(1)\n",
    "corr_TSLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-24T01:00:15.517926Z",
     "start_time": "2022-10-24T01:00:15.485940Z"
    }
   },
   "outputs": [],
   "source": [
    "corr_TSLA['MaxCorrTicker'].value_counts(),corr_TSLA['MaxCorrTicker'].value_counts()/len(corr_TSLA['MaxCorrTicker'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "    \n",
    "From above we can see that in nearly 75% of all 6-month rolling window, either `GOOG` or `SNAP` has the highest correlation with `TSLA`. In addition, there's still 25% of all time where `GM` or `F` has the highest correlation with `TSLA`, and the highest correlation is always significant and positive. Thus, we can conclude that `TSLA` behaves more like a technology services company, but still has the characteristics of car manufacturing companies. \n",
    "    \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "We can also use construct linear regression models to analyse the performance characteristics of returns series.  For example, given that the `SPY` represents (very approximately) the performance of market for Large Cap US companies, if we think that the performance of `UBER` can be described as some function of the return of the broader markets, an idiosyncratic component, plus some noise, we can construct a linear relationship between `SPY` and `UBER` specified by\n",
    "\n",
    "$$Ret_{UBER} = \\alpha + \\beta * Ret_{SPY} + \\epsilon$$\n",
    "\n",
    "Using whatever method you feel most comfortable with, please perform a linear regression analysis on this hypothesized relationship using the full period of available data, tabulating your full results below.  **While we do not have specific prompts for written comments, please be prepared to briefly discuss the results of the linear regression analysis, including your methods**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-23T17:41:17.587237Z",
     "start_time": "2022-10-23T17:41:17.573246Z"
    }
   },
   "outputs": [],
   "source": [
    "from statsmodels.stats.diagnostic import het_white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-23T15:19:26.092684Z",
     "start_time": "2022-10-23T15:19:25.840829Z"
    }
   },
   "outputs": [],
   "source": [
    "# Linear regression model fitting\n",
    "reg_subset = Ret[['UBER','SPY']].dropna()\n",
    "Y = reg_subset['UBER']\n",
    "X = reg_subset['SPY']\n",
    "X = sm.add_constant(X)\n",
    "model = sm.OLS(Y,X).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-23T17:41:24.827091Z",
     "start_time": "2022-10-23T17:41:24.783117Z"
    }
   },
   "outputs": [],
   "source": [
    "# Test for heteroskedasticity\n",
    "white_test = het_white(model.resid,model.model.exog)\n",
    "\n",
    "labels = ['Test Statistic', 'Test Statistic p-value', 'F-Statistic', 'F-Test p-value']\n",
    "print(dict(zip(labels, white_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-23T15:19:15.648497Z",
     "start_time": "2022-10-23T15:19:15.608518Z"
    }
   },
   "source": [
    "<font color='blue'>\n",
    "    \n",
    "From the stats above, there's no significant heteroskedasticity in the residuals.\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for residual autocorrelation\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "plot_acf(model.resid);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "From ACF plot, there are autocorrelation when lag = 4,10 and 12. Thus, an AR model or seasonal model might be a better fit for this regression.             \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "Check for the beta from another calculation method:\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_UBER_SPY = Ret[['UBER','SPY']].dropna().corr().iloc[0,1]\n",
    "beta_theoretical = corr_UBER_SPY*Ret[['UBER','SPY']].dropna()['UBER'].std()/Ret[['UBER','SPY']].dropna()['SPY'].std()\n",
    "round(beta_theoretical,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.graphics.plot_fit(model,1, vlines=False);"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
